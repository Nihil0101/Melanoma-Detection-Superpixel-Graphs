{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sye32awWnHzt",
        "BSWyA1bCnJGj",
        "vkXEVCtJocJg",
        "ZFxRz9IyqO0I",
        "TYrLgDDruaj2",
        "-ykV7Yvyb_6C"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "V7g8_xtyIOrO",
        "outputId": "fa7e416a-4914-4d83-8b8f-0b479d62a857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload model configuration file in pickle or joblib format.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4367acb6-a6fa-48a8-93d4-137bd37040cd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4367acb6-a6fa-48a8-93d4-137bd37040cd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving RandomForestClassifier.pkl to RandomForestClassifier.pkl\n",
            "User uploaded file \"RandomForestClassifier.pkl\" with length 687063 bytes\n",
            "Model configuration file ok: RandomForestClassifier.pkl\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"Upload model configuration file in pickle or joblib format.\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n",
        "  _, ext = os.path.splitext(fn)\n",
        "\n",
        "  # If the file is not a pickle or joblib file, delete it\n",
        "  if ext.lower() != '.pkl' and ext.lower() != '.joblib':\n",
        "      print(f\"Deleting non-model configuration file: {fn}\")\n",
        "      os.remove(fn)\n",
        "  else:\n",
        "      print(f\"Model configuration file ok: {fn}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install py7zr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWK69V-BQph7",
        "outputId": "4b27a5e3-ad9d-4c77-8d20-a57cc79ad7cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting py7zr\n",
            "  Downloading py7zr-0.21.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m830.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting texttable (from py7zr)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pycryptodomex>=3.16.0 (from py7zr)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzstd>=0.15.9 (from py7zr)\n",
            "  Downloading pyzstd-0.15.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (411 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.2/411.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyppmd<1.2.0,>=1.1.0 (from py7zr)\n",
            "  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybcj<1.1.0,>=1.0.0 (from py7zr)\n",
            "  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr)\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr)\n",
            "  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli>=1.1.0 (from py7zr)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from py7zr) (5.9.5)\n",
            "Installing collected packages: texttable, brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, py7zr\n",
            "Successfully installed brotli-1.1.0 inflate64-1.0.0 multivolumefile-0.2.3 py7zr-0.21.0 pybcj-1.0.2 pycryptodomex-3.20.0 pyppmd-1.1.0 pyzstd-0.15.10 texttable-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import py7zr\n",
        "print(\"Upload archive file of images with masks (10 mb max) in 7z format.\")\n",
        "\n",
        "# Upload the files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Create a new folder\n",
        "root_folder = 'Images/'\n",
        "os.makedirs(root_folder, exist_ok=True)\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n",
        "  _, ext = os.path.splitext(fn)\n",
        "\n",
        "  if ext.lower() != '.7z':\n",
        "      print(f\"Deleting non-archive file: {fn}\")\n",
        "      os.remove(fn)\n",
        "  else:\n",
        "      print(f\"7z file ok: {fn}\")\n",
        "      new_path = os.path.join(root_folder, fn)\n",
        "      os.rename(fn, new_path)\n",
        "\n",
        "      # Extract the 7z file\n",
        "      with py7zr.SevenZipFile(new_path, mode='r') as z:\n",
        "          z.extractall(path=root_folder)\n",
        "\n",
        "      print(f\"Extracted archive file: {fn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "O_Thpr1RPDc2",
        "outputId": "48fe3168-19b3-43c5-ac08-5bbfcd53bafb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload archive file of images with masks (10 mb max) in 7z format.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-133a7a0c-9a8e-4164-a83c-d9e176c08938\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-133a7a0c-9a8e-4164-a83c-d9e176c08938\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test image.7z to test image.7z\n",
            "User uploaded file \"test image.7z\" with length 1880869 bytes\n",
            "7z file ok: test image.7z\n",
            "Extracted archive file: test image.7z\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "your_directory_path = '/content/Images'\n",
        "folder_names = []\n",
        "for folder_name in os.listdir(your_directory_path):\n",
        "    if os.path.isdir(os.path.join(your_directory_path, folder_name)):\n",
        "        folder_names.append(folder_name)"
      ],
      "metadata": {
        "id": "Wfi0HaXaSnRX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.segmentation import slic, mark_boundaries\n",
        "from skimage.util import img_as_float\n",
        "from skimage import io, color, morphology,img_as_ubyte\n",
        "from skimage.transform import resize\n",
        "from skimage.feature import graycomatrix,graycoprops\n",
        "from skimage import exposure\n",
        "from skimage.measure import regionprops\n",
        "\n",
        "from scipy.spatial import Delaunay\n",
        "from scipy.linalg import eigh\n",
        "from scipy.stats import entropy,skew\n",
        "\n",
        "import networkx as nx\n",
        "import cv2\n",
        "import math\n",
        "import pywt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
        "import pickle\n",
        "import fnmatch\n",
        "\n",
        "import warnings\n",
        "# Ignore pandas warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "cEUUlvmIWO1Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_graph(img_path, mask_path):\n",
        "  image = img_as_float(io.imread(img_path))\n",
        "  image = cv2.resize(image, (750, 750))  # Resize the image to 750x750 pixels\n",
        "  mask = cv2.imread(mask_path,cv2.IMREAD_GRAYSCALE)\n",
        "  mask = cv2.resize(mask, (750, 750))\n",
        "\n",
        "  # Apply SLIC with masking and extract (approximately) the supplied number of segments\n",
        "  segments = slic(image, n_segments=20, sigma=18, mask=mask)\n",
        "  middle_pixels = np.zeros((np.max(segments) + 1, 2))\n",
        "\n",
        "  # Calculate a non-border valid pixel for each segment\n",
        "  for label in np.unique(segments):\n",
        "      if label == 0:  # Skip segment 0\n",
        "          continue\n",
        "\n",
        "      mask = segments == label\n",
        "      valid_pixels = np.where(mask & mask)  # Select valid pixels (mask == 1)\n",
        "      if len(valid_pixels[0]) > 0:  # Check if there are valid pixels in the region\n",
        "          valid_rows, valid_cols = valid_pixels\n",
        "          distances_to_border = np.minimum(\n",
        "              np.minimum(valid_rows, mask.shape[0] - 1 - valid_rows),\n",
        "              np.minimum(valid_cols, mask.shape[1] - 1 - valid_cols)\n",
        "          )\n",
        "          non_border_indices = np.where(distances_to_border >= 20)[0]  # Minimum distance from border increased to 20\n",
        "          if len(non_border_indices) > 0:\n",
        "              random_index = np.random.choice(non_border_indices)  # Choose a random index\n",
        "              random_pixel = (valid_cols[random_index], valid_rows[random_index])  # Note the change in ordering\n",
        "              middle_pixels[label] = random_pixel\n",
        "\n",
        "  # Calculate Delaunay triangulation based on the middle pixels (excluding segment 0)\n",
        "  valid_middle_pixels = middle_pixels[middle_pixels[:, 0] != 0]  # Exclude segment 0\n",
        "  triangulation = Delaunay(valid_middle_pixels)\n",
        "\n",
        "  G = nx.Graph()\n",
        "  for simplex in triangulation.simplices:\n",
        "      G.add_edge(simplex[0], simplex[1])\n",
        "      G.add_edge(simplex[1], simplex[2])\n",
        "      G.add_edge(simplex[2], simplex[0])\n",
        "  return G"
      ],
      "metadata": {
        "id": "HxmJOfNCWcVr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(img_path, mask_path):\n",
        "  #graph domain\n",
        "  G = generate_graph(img_path, mask_path)\n",
        "  ge = global_efficiency(G)\n",
        "  local_efficiencies = local_efficiency(G)#for node in graph.nodes():\n",
        "  local_clustering_coeffs = local_clustering_coefficient(G)#for node in graph.nodes():\n",
        "  nodal_strengths = nodal_strength(G)#for node in graph.nodes()\n",
        "  nodal_betweenness = nodal_betweenness_centrality(G)\n",
        "  sorted_nodes_nodal_betweenness = sorted(nodal_betweenness.items(), key=lambda x: x[0])\n",
        "  closeness_centralities = closeness_centrality(G)\n",
        "  sorted_nodes_closeness_centralities = sorted(closeness_centralities.items(), key=lambda x: x[0])\n",
        "  eccentricities = nx.eccentricity(G)\n",
        "  sorted_nodes_eccentricities = sorted(eccentricities.items(), key=lambda x: x[0])\n",
        "  char_path_length = characteristic_path_length(G)\n",
        "  global_clustering_coeff = global_clustering_coefficient(local_clustering_coeffs)\n",
        "  graph_density_value = graph_density(G)\n",
        "  global_assortativity_value = global_assortativity(G)\n",
        "\n",
        "  #freq domain\n",
        "  f_h = calc_f_hat(G)\n",
        "  ffd_energy,ffd_power,ffd_spectral_entropy,ffd_amplitude = finish_frequency_domain(G)\n",
        "\n",
        "  #geometry domain\n",
        "  geo_feats = calc_geo_feats(mask_path)\n",
        "\n",
        "  # COLOR FEATURES\n",
        "  image = cv2.imread(img_path)\n",
        "  image = cv2.resize(image, (750, 750))#TODO: optimize to reuse existing\n",
        "  mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "  mask = cv2.resize(mask, (750, 750))#TODO: optimize to reuse existing\n",
        "  # Mask the grayscale image using the binary mask\n",
        "  masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
        "\n",
        "  # Calculate the mean, variance, and standard deviation of the masked image\n",
        "  mean_value = np.mean(masked_image)\n",
        "  variance_value = np.var(masked_image)\n",
        "  std_deviation = np.sqrt(variance_value)\n",
        "\n",
        "  # Load the image\n",
        "  image = cv2.imread(img_path)\n",
        "  image = cv2.resize(image, (750, 750))#TODO: optimize to reuse existing\n",
        "\n",
        "  # Split the image into color channels (BGR channels for OpenCV)\n",
        "  blue_channel, green_channel, red_channel = cv2.split(image)\n",
        "\n",
        "  # Convert the image to HSV color space\n",
        "  hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "  hue_channel, sat_channel, val_channel = cv2.split(hsv_image)\n",
        "\n",
        "  # Convert the image to CIE Lab color space\n",
        "  lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2Lab)\n",
        "  lab_l_channel, lab_a_channel, lab_b_channel = cv2.split(lab_image)\n",
        "\n",
        "  # Convert the image to CIE Luv color space\n",
        "  luv_image = cv2.cvtColor(image, cv2.COLOR_BGR2Luv)\n",
        "  luv_l_channel, luv_u_channel, luv_v_channel = cv2.split(luv_image)\n",
        "\n",
        "  # Calculate statistics for each channel in different color spaces\n",
        "  # HSV Color Space\n",
        "  avg_hue, var_hue, std_dev_hue, min_hue, max_hue, skew_hue = calculate_stats(hue_channel)\n",
        "  avg_sat, var_sat, std_dev_sat, min_sat, max_sat, skew_sat = calculate_stats(sat_channel)\n",
        "  avg_val, var_val, std_dev_val, min_val, max_val, skew_val = calculate_stats(val_channel)\n",
        "\n",
        "  # CIE Lab Color Space\n",
        "  avg_lab_l, var_lab_l, std_dev_lab_l, min_lab_l, max_lab_l, skew_lab_l = calculate_stats(lab_l_channel)\n",
        "  avg_lab_a, var_lab_a, std_dev_lab_a, min_lab_a, max_lab_a, skew_lab_a = calculate_stats(lab_a_channel)\n",
        "  avg_lab_b, var_lab_b, std_dev_lab_b, min_lab_b, max_lab_b, skew_lab_b = calculate_stats(lab_b_channel)\n",
        "\n",
        "  # CIE Luv Color Space\n",
        "  avg_luv_l, var_luv_l, std_dev_luv_l, min_luv_l, max_luv_l, skew_luv_l = calculate_stats(luv_l_channel)\n",
        "  avg_luv_u, var_luv_u, std_dev_luv_u, min_luv_u, max_luv_u, skew_luv_u = calculate_stats(luv_u_channel)\n",
        "  avg_luv_v, var_luv_v, std_dev_luv_v, min_luv_v, max_luv_v, skew_luv_v = calculate_stats(luv_v_channel)\n",
        "\n",
        "  # Calculate statistics for each channel\n",
        "  avg_blue, var_blue, std_dev_blue, min_blue, max_blue, skew_blue = calculate_stats(blue_channel)\n",
        "  avg_green, var_green, std_dev_green, min_green, max_green, skew_green = calculate_stats(green_channel)\n",
        "  avg_red, var_red, std_dev_red, min_red, max_red, skew_red = calculate_stats(red_channel)\n",
        "  # COLOR FEATURES END\n",
        "\n",
        "  #Hausdorff\n",
        "  image = cv2.imread(img_path)\n",
        "  image = cv2.resize(image, (750, 750))#TODO: optimize to reuse existing\n",
        "  r_channel, g_channel, b_channel_rgb = cv2.split(image)\n",
        "\n",
        "  luv_image = cv2.cvtColor(image.astype(np.float32), cv2.COLOR_BGR2LUV)\n",
        "  l_channel_luv, u_channel, v_channel_luv = cv2.split(luv_image)\n",
        "\n",
        "  lab_image = cv2.cvtColor(image.astype(np.float32), cv2.COLOR_BGR2LAB)\n",
        "  l_channel_lab, a_channel, b_channel_lab = cv2.split(lab_image)\n",
        "\n",
        "  hsv_image = cv2.cvtColor(image.astype(np.float32), cv2.COLOR_BGR2HSV)\n",
        "  h_channel, s_channel, v_channel_hsv = cv2.split(hsv_image)\n",
        "\n",
        "  Hausdorff_rgb_r = Get_Hausdorff_dim(r_channel)\n",
        "  Hausdorff_rgb_g =Get_Hausdorff_dim(g_channel)\n",
        "  Hausdorff_rgb_b =Get_Hausdorff_dim(b_channel_rgb)\n",
        "\n",
        "  Hausdorff_luv_l = Get_Hausdorff_dim(l_channel_luv)\n",
        "  Hausdorff_luv_u = Get_Hausdorff_dim(u_channel)\n",
        "  Hausdorff_luv_v = Get_Hausdorff_dim(v_channel_luv)\n",
        "\n",
        "  Hausdorff_lab_l = Get_Hausdorff_dim(l_channel_lab)\n",
        "  Hausdorff_lab_a = Get_Hausdorff_dim(a_channel)\n",
        "  Hausdorff_lab_b = Get_Hausdorff_dim(b_channel_lab)\n",
        "\n",
        "  Hausdorff_hsv_h = Get_Hausdorff_dim(h_channel)\n",
        "  Hausdorff_hsv_s = Get_Hausdorff_dim(s_channel)\n",
        "  Hausdorffk_hsv_v = Get_Hausdorff_dim(v_channel_hsv)\n",
        "  #Hausdorff end\n",
        "\n",
        "  #DWT\n",
        "  image = cv2.imread(img_path)\n",
        "  image = cv2.resize(image, (750, 750))#TODO: optimize to reuse existing\n",
        "  r_channel, g_channel, b_channel_rgb = cv2.split(image)\n",
        "\n",
        "  luv_image = cv2.cvtColor(image, cv2.COLOR_BGR2LUV)\n",
        "  l_channel_luv, u_channel, v_channel_luv = cv2.split(luv_image)\n",
        "\n",
        "  lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "  l_channel_lab, a_channel, b_channel_lab = cv2.split(lab_image)\n",
        "\n",
        "  hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "  h_channel, s_channel, v_channel_hsv = cv2.split(hsv_image)\n",
        "\n",
        "  tmpd1 = Get_waves(r_channel, g_channel, b_channel_rgb,'rgb_')\n",
        "  tmpd2 = Get_waves(l_channel_luv, u_channel, v_channel_luv,'luv_')\n",
        "  tmpd3 = Get_waves(l_channel_lab, a_channel, b_channel_lab,'lab_')\n",
        "  tmpd4 = Get_waves(h_channel, s_channel, v_channel_hsv,'hsv')\n",
        "  #DWT end\n",
        "\n",
        "  #GLCM\n",
        "  Haralick_rgb = Calc_GLCMs(['R_','G_','B_'],r_channel,g_channel,b_channel_rgb)\n",
        "  Haralick_luv = Calc_GLCMs(['L_','U_','V_'],l_channel_luv,u_channel,v_channel_luv)\n",
        "  Haralick_lab_l = Calc_GLCMs(['L_','A_','B_'],l_channel_lab,a_channel,b_channel_lab)\n",
        "  Haralick_hsv_h = Calc_GLCMs(['H_','S_','V_'],h_channel,s_channel,v_channel_hsv)\n",
        "  #GLCM end\n",
        "  geo_data = {}\n",
        "\n",
        "\n",
        "  geo_data = {'geo_area_1': [geo_feats[0]],\n",
        "          'geo_perimeter_1': [geo_feats[1]],\n",
        "          'geo_equivalent_diameter_1': [geo_feats[2]],\n",
        "          'geo_compactness_1': [geo_feats[3]],\n",
        "          'geo_circularity_1': [geo_feats[4]],\n",
        "          'geo_solidity_1': [geo_feats[5]],\n",
        "          'geo_rectangularity_1': [geo_feats[6]],\n",
        "          'geo_aspect_ratio_1': [geo_feats[7]],\n",
        "          'geo_eccentricity_1': [geo_feats[8]],\n",
        "          }\n",
        "\n",
        "  color_stats_data = {\n",
        "      'avg_hue': avg_hue, 'var_hue': var_hue, 'std_dev_hue': std_dev_hue, 'min_hue': min_hue, 'max_hue': max_hue, 'skew_hue': skew_hue,\n",
        "      'avg_sat': avg_sat, 'var_sat': var_sat, 'std_dev_sat': std_dev_sat, 'min_sat': min_sat, 'max_sat': max_sat, 'skew_sat': skew_sat,\n",
        "      'avg_val': avg_val, 'var_val': var_val, 'std_dev_val': std_dev_val, 'min_val': min_val, 'max_val': max_val, 'skew_val': skew_val,\n",
        "      'avg_lab_l': avg_lab_l, 'var_lab_l': var_lab_l, 'std_dev_lab_l': std_dev_lab_l, 'min_lab_l': min_lab_l, 'max_lab_l': max_lab_l, 'skew_lab_l': skew_lab_l,\n",
        "      'avg_lab_a': avg_lab_a, 'var_lab_a': var_lab_a, 'std_dev_lab_a': std_dev_lab_a, 'min_lab_a': min_lab_a, 'max_lab_a': max_lab_a, 'skew_lab_a': skew_lab_a,\n",
        "      'avg_lab_b': avg_lab_b, 'var_lab_b': var_lab_b, 'std_dev_lab_b': std_dev_lab_b, 'min_lab_b': min_lab_b, 'max_lab_b': max_lab_b, 'skew_lab_b': skew_lab_b,\n",
        "      'avg_luv_l': avg_luv_l, 'var_luv_l': var_luv_l, 'std_dev_luv_l': std_dev_luv_l, 'min_luv_l': min_luv_l, 'max_luv_l': max_luv_l, 'skew_luv_l': skew_luv_l,\n",
        "      'avg_luv_u': avg_luv_u, 'var_luv_u': var_luv_u, 'std_dev_luv_u': std_dev_luv_u, 'min_luv_u': min_luv_u, 'max_luv_u': max_luv_u, 'skew_luv_u': skew_luv_u,\n",
        "      'avg_luv_v': avg_luv_v, 'var_luv_v': var_luv_v, 'std_dev_luv_v': std_dev_luv_v, 'min_luv_v': min_luv_v, 'max_luv_v': max_luv_v, 'skew_luv_v': skew_luv_v,\n",
        "      'avg_blue': avg_blue, 'var_blue': var_blue, 'std_dev_blue': std_dev_blue, 'min_blue': min_blue, 'max_blue': max_blue, 'skew_blue': skew_blue,\n",
        "      'avg_green': avg_green, 'var_green': var_green, 'std_dev_green': std_dev_green, 'min_green': min_green, 'max_green': max_green, 'skew_green': skew_green,\n",
        "      'avg_red': avg_red, 'var_red': var_red, 'std_dev_red': std_dev_red, 'min_red': min_red, 'max_red': max_red, 'skew_red': skew_red\n",
        "  }\n",
        "\n",
        "  hausdorff_data = {\n",
        "      'Hausdorff_rgb_r': Hausdorff_rgb_r,\n",
        "      'Hausdorff_rgb_g': Hausdorff_rgb_g,\n",
        "      'Hausdorff_rgb_b': Hausdorff_rgb_b,\n",
        "      'Hausdorff_luv_l': Hausdorff_luv_l,\n",
        "      'Hausdorff_luv_u': Hausdorff_luv_u,\n",
        "      'Hausdorff_luv_v': Hausdorff_luv_v,\n",
        "      'Hausdorff_lab_l': Hausdorff_lab_l,\n",
        "      'Hausdorff_lab_a': Hausdorff_lab_a,\n",
        "      'Hausdorff_lab_b': Hausdorff_lab_b,\n",
        "      'Hausdorff_hsv_h': Hausdorff_hsv_h,\n",
        "      'Hausdorff_hsv_s': Hausdorff_hsv_s,\n",
        "      'Hausdorffk_hsv_v': Hausdorffk_hsv_v\n",
        "  }\n",
        "\n",
        "  # Create a simple dataframe with one column and one value\n",
        "  data = {'global_efficiency': [ge],\n",
        "          'char_path_length': [char_path_length],\n",
        "          'global_clustering_coeff': [global_clustering_coeff],\n",
        "          'graph_density_value': [graph_density_value],\n",
        "          'global_assortativity_value': [global_assortativity_value],\n",
        "          #\n",
        "          'frequency_domain_energy': [ffd_energy],\n",
        "          'frequency_domain_power': [ffd_power],\n",
        "          'frequency_domain_entropy': [ffd_spectral_entropy],\n",
        "          'frequency_domain_amplitude': [ffd_amplitude],\n",
        "          }\n",
        "\n",
        "  data.update(geo_data)\n",
        "  data.update(color_stats_data)\n",
        "  data.update(hausdorff_data)\n",
        "  data.update(tmpd1)\n",
        "  data.update(tmpd2)\n",
        "  data.update(tmpd3)\n",
        "  data.update(tmpd4)\n",
        "\n",
        "  data.update(Haralick_rgb)\n",
        "  data.update(Haralick_luv)\n",
        "  data.update(Haralick_lab_l)\n",
        "  data.update(Haralick_hsv_h)\n",
        "\n",
        "  df = pd.DataFrame(data)\n",
        "\n",
        "  # Append each value as a new column\n",
        "  for i, value in enumerate(local_efficiencies, start=1):\n",
        "      df[f'local_efficiency_node_{i}'] = [value]\n",
        "\n",
        "  # Append each value as a new column\n",
        "  for i, value in enumerate(local_clustering_coeffs, start=1):\n",
        "      df[f'local_clustering_coef_node_{i}'] = [value]\n",
        "\n",
        "  # Append each value as a new column\n",
        "  for i, value in enumerate(nodal_strengths, start=1):\n",
        "      df[f'strength_node_{i}'] = [value]\n",
        "\n",
        "  # Append each value as a new column\n",
        "  for i, value in sorted_nodes_nodal_betweenness:\n",
        "      df[f'nodal_betweenness_node_{i+1}'] = [value]\n",
        "\n",
        "  # Append each value as a new column\n",
        "  for i, value in sorted_nodes_closeness_centralities:\n",
        "      df[f'closeness_centrality_node_{i+1}'] = [value]\n",
        "\n",
        "  # Append each value as a new column\n",
        "  for i, value in sorted_nodes_eccentricities:\n",
        "      df[f'eccentricity_node_{i+1}'] = [value]\n",
        "\n",
        "  for i, value in enumerate(f_h.tolist(),start=1):\n",
        "      df[f'f_hat{i}'] = [value]\n",
        "\n",
        "  # Write the dataframe object into CSV file\n",
        "  df.to_csv('test.csv', index=None, header=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "JEyY9-JlVt8B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GLOBAL METHODS"
      ],
      "metadata": {
        "id": "sye32awWnHzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def characteristic_path_length(graph):\n",
        "    total_path_length = 0\n",
        "    num_pairs = 0\n",
        "\n",
        "    for node_i in graph.nodes():\n",
        "        for node_j in graph.nodes():\n",
        "            if node_i != node_j:\n",
        "                shortest_path_length = nx.shortest_path_length(graph, source=node_i, target=node_j)\n",
        "                total_path_length += shortest_path_length\n",
        "                num_pairs += 1\n",
        "\n",
        "    characteristic_length = total_path_length / num_pairs\n",
        "    return characteristic_length"
      ],
      "metadata": {
        "id": "AALfbxKInbMT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate global efficiency for the graph\n",
        "def global_efficiency(graph):\n",
        "    total_efficiency = 0.0\n",
        "    num_pairs = 0\n",
        "\n",
        "    for source_node in graph.nodes():\n",
        "        for target_node in graph.nodes():\n",
        "            if source_node != target_node:\n",
        "                try:\n",
        "                    sp_length = nx.shortest_path_length(graph, source=source_node, target=target_node)\n",
        "                    total_efficiency += 1.0 / sp_length\n",
        "                    num_pairs += 1\n",
        "                except nx.NetworkXNoPath:\n",
        "                    # Nodes are not connected\n",
        "                    pass\n",
        "\n",
        "    if num_pairs > 0:\n",
        "        global_efficiency = total_efficiency / num_pairs\n",
        "        return global_efficiency\n",
        "    else:\n",
        "        return 0.0"
      ],
      "metadata": {
        "id": "bo1fGwXgnBov"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def global_clustering_coefficient(local_clustering_coeffs):\n",
        "    num_nodes = len(local_clustering_coeffs)\n",
        "    global_coefficient = sum(local_clustering_coeffs) / num_nodes\n",
        "    return global_coefficient"
      ],
      "metadata": {
        "id": "dibDPv-rngpO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate graph density for the unweighted graph\n",
        "def graph_density(graph):\n",
        "    num_edges = graph.number_of_edges()\n",
        "    num_nodes = graph.number_of_nodes()\n",
        "    max_possible_edges = num_nodes * (num_nodes - 1)\n",
        "\n",
        "    density = (2 * num_edges) / max_possible_edges\n",
        "    return density"
      ],
      "metadata": {
        "id": "VnEaSLOunn-7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def global_assortativity(graph):\n",
        "    assortativity = nx.degree_assortativity_coefficient(graph)\n",
        "    return assortativity"
      ],
      "metadata": {
        "id": "16NVWzrbnr_X"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOCAL METHODS"
      ],
      "metadata": {
        "id": "BSWyA1bCnJGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate local efficiency for each node\n",
        "def local_efficiency(graph):\n",
        "    local_efficiencies = []\n",
        "\n",
        "    # Calculate global efficiency for the entire graph\n",
        "    global_efficiency_value = global_efficiency(graph)\n",
        "\n",
        "    for node in graph.nodes():\n",
        "        neighbors = list(graph.neighbors(node))\n",
        "\n",
        "        # Create subgraph of neighbors\n",
        "        subgraph = graph.subgraph(neighbors)\n",
        "\n",
        "        # Calculate the global efficiency for the subgraph\n",
        "        subgraph_efficiency = global_efficiency(subgraph)\n",
        "\n",
        "        # Calculate local efficiency as a ratio of subgraph_efficiency to global_efficiency_value\n",
        "        if global_efficiency_value != 0:\n",
        "            local_efficiency_value = subgraph_efficiency / global_efficiency_value\n",
        "        else:\n",
        "            local_efficiency_value = 0.0\n",
        "\n",
        "        local_efficiencies.append(local_efficiency_value)\n",
        "\n",
        "    return local_efficiencies"
      ],
      "metadata": {
        "id": "QwNn2i43nHI2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def local_clustering_coefficient(graph):\n",
        "    local_clustering_coefficients = []\n",
        "\n",
        "    for node in graph.nodes():\n",
        "        neighbors = list(graph.neighbors(node))\n",
        "        degree = graph.degree(node)\n",
        "\n",
        "        # Count the number of edges between neighbors\n",
        "        edges_within_neighbors = sum(1 for u, v in graph.subgraph(neighbors).edges() if u in neighbors and v in neighbors)\n",
        "\n",
        "        # Calculate local clustering coefficient for the node\n",
        "        if degree > 1:\n",
        "            lcc = (2 * edges_within_neighbors) / (degree * (degree - 1))\n",
        "        else:\n",
        "            lcc = 0.0\n",
        "\n",
        "        local_clustering_coefficients.append(lcc)\n",
        "\n",
        "    return local_clustering_coefficients"
      ],
      "metadata": {
        "id": "f2DpD4GVoDoy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nodal_strength(graph):\n",
        "    nodal_strengths = [graph.degree(node) for node in graph.nodes()]\n",
        "    return nodal_strengths"
      ],
      "metadata": {
        "id": "Mvjz8-r8nTt2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate nodal betweenness centrality for each node\n",
        "def nodal_betweenness_centrality(graph):\n",
        "    nodal_betweenness = nx.betweenness_centrality(graph)\n",
        "    return nodal_betweenness"
      ],
      "metadata": {
        "id": "4Q0-eWkEnWa_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def closeness_centrality(graph):\n",
        "    closeness_centralities = nx.closeness_centrality(graph)\n",
        "    return closeness_centralities"
      ],
      "metadata": {
        "id": "smNc_wImnXLj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate eccentricity for each node\n",
        "def eccentricity(graph):\n",
        "    eccentricities = nx.eccentricity(graph)\n",
        "    return eccentricities"
      ],
      "metadata": {
        "id": "XLqg21H3oQjN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features in the Frequency Domain"
      ],
      "metadata": {
        "id": "vkXEVCtJocJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_adjacency_matrix(graph):\n",
        "    num_nodes = graph.number_of_nodes()\n",
        "    adjacency_matrix = np.zeros((num_nodes, num_nodes), dtype=int)\n",
        "\n",
        "    for edge in graph.edges():\n",
        "        node_i, node_j = edge\n",
        "        adjacency_matrix[node_i, node_j] = 1\n",
        "        adjacency_matrix[node_j, node_i] = 1  # For undirected graph\n",
        "\n",
        "    return adjacency_matrix"
      ],
      "metadata": {
        "id": "0rWR8mL0oer8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_f_hat(G):\n",
        "  adjacency_matrix = create_adjacency_matrix(G)\n",
        "  #Calculate the degree of vertices\n",
        "  degrees = dict(G.degree())\n",
        "  # Construct the degree matrix\n",
        "  degree_matrix = np.diag(list(degrees.values()))\n",
        "  # Calculate shortest path distances between connected vertices\n",
        "  shortest_paths = nx.shortest_path_length(G)\n",
        "  L = degree_matrix - adjacency_matrix\n",
        "  # Calculate eigenvalues (Λ) and eigenvectors (U)\n",
        "  eigenvalues, eigenvectors = np.linalg.eigh(L)\n",
        "  return np.dot(eigenvalues, eigenvectors)"
      ],
      "metadata": {
        "id": "2SMWAFy5ZG16"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def finish_frequency_domain(G):\n",
        "  # Compute the Laplacian matrix\n",
        "  adjacency_matrix = create_adjacency_matrix(G)\n",
        "  degree_matrix = np.diag(adjacency_matrix.sum(axis=1))\n",
        "  laplacian_matrix = degree_matrix - adjacency_matrix\n",
        "\n",
        "  # Compute the eigenvalues\n",
        "  eigenvalues = eigh(laplacian_matrix, eigvals_only=True)\n",
        "\n",
        "  # Normalize the eigenvalues for a probability distribution\n",
        "  eigenvalues /= eigenvalues.sum()\n",
        "\n",
        "  # Energy is the sum of the square of the eigenvalues\n",
        "  energy = np.sum(np.square(eigenvalues))\n",
        "  # Power is the energy divided by the size of the eigenvalues\n",
        "  power = energy / len(eigenvalues)\n",
        "  # Entropy is a statistical measure of randomness\n",
        "  spectral_entropy = entropy(eigenvalues)\n",
        "  # Amplitude is the square root of the energy\n",
        "  amplitude = np.sqrt(energy)\n",
        "  return energy,power,spectral_entropy,amplitude"
      ],
      "metadata": {
        "id": "N8bmnc7mZajK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Geometry-based graph signals"
      ],
      "metadata": {
        "id": "ZFxRz9IyqO0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to calculate contour properties\n",
        "def calculate_contour_properties(contour,binary_image):\n",
        "    area = cv2.contourArea(contour)\n",
        "    perimeter = cv2.arcLength(contour, True)\n",
        "\n",
        "    if area > 0:\n",
        "        equivalent_diameter = np.sqrt(4 * area / np.pi)\n",
        "        compactness = (perimeter ** 2) / (4 * np.pi * area)\n",
        "        circularity = (4 * np.pi * area) / (perimeter ** 2)\n",
        "        ellipse = cv2.fitEllipse(contour)\n",
        "\n",
        "        # The second element of the tuple returned by cv2.fitEllipse() contains the lengths of the major and minor axes\n",
        "        major_axis = max(ellipse[1])\n",
        "        minor_axis = min(ellipse[1])\n",
        "\n",
        "        # Calculate the eccentricity\n",
        "        eccentricity = np.sqrt(1 - (minor_axis / major_axis)**2) if major_axis > 0 else 0\n",
        "    else:\n",
        "        equivalent_diameter = 0\n",
        "        compactness = 0\n",
        "        circularity = 0\n",
        "        eccentricity = 0\n",
        "\n",
        "    hull = cv2.convexHull(contour)\n",
        "    solidity = (area / cv2.contourArea(hull)) if cv2.contourArea(hull) > 0 else 0\n",
        "\n",
        "    x, y, w, h = cv2.boundingRect(contour)\n",
        "    rectangularity = area / (w * h)\n",
        "    aspect_ratio = w / h\n",
        "\n",
        "    return area, perimeter, equivalent_diameter, compactness, circularity, solidity, rectangularity, aspect_ratio, eccentricity"
      ],
      "metadata": {
        "id": "Mvm_RYHLrmrD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_geo_feats(mask_path):\n",
        "  image = cv2.imread(mask_path)\n",
        "  image = cv2.resize(image, (750, 750))#TODO: optimize to reuse existing\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Threshold or segment the image as needed\n",
        "  ret, binary_image = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "  # Find contours\n",
        "  contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  contour_image = image.copy()\n",
        "\n",
        "  geo_feats_list = []\n",
        "  for contour in contours:\n",
        "      if cv2.contourArea(contour) > 100:\n",
        "        area, perimeter, equivalent_diameter, compactness, circularity, solidity, rectangularity, aspect_ratio, eccentricity = calculate_contour_properties(contour,binary_image)\n",
        "        geo_feats_list.append(area)\n",
        "        geo_feats_list.append(perimeter)\n",
        "        geo_feats_list.append(equivalent_diameter)\n",
        "        geo_feats_list.append(compactness)\n",
        "        geo_feats_list.append(circularity)\n",
        "        geo_feats_list.append(solidity)\n",
        "        geo_feats_list.append(rectangularity)\n",
        "        geo_feats_list.append(aspect_ratio)\n",
        "        geo_feats_list.append(eccentricity)\n",
        "        break#image should contain only 1 contour\n",
        "\n",
        "  return geo_feats_list"
      ],
      "metadata": {
        "id": "wOCB5mKSZRXy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate statistics for a given channel\n",
        "def calculate_stats(channel):\n",
        "    average = np.mean(channel)\n",
        "    variance = np.var(channel)\n",
        "    std_deviation = np.std(channel)\n",
        "    min_val = np.min(channel)\n",
        "    max_val = np.max(channel)\n",
        "    skewness = skew(channel, axis=None)\n",
        "    return average, variance, std_deviation, min_val, max_val, skewness"
      ],
      "metadata": {
        "id": "JZ16D5tsaNcC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hausdorff"
      ],
      "metadata": {
        "id": "TYrLgDDruaj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Get_Hausdorff_dim(channel):\n",
        "  pixels = []\n",
        "  image = channel\n",
        "  for i in range(image.shape[0]):\n",
        "      for j in range(image.shape[1]):\n",
        "          if image[i, j] > 0:\n",
        "              pixels.append((i, j))\n",
        "\n",
        "  Lx = image.shape[1]\n",
        "  Ly = image.shape[0]\n",
        "  pixels = np.array(pixels)\n",
        "  scales = np.logspace(0.01, 1, num=10, endpoint=False, base=2)\n",
        "  Ns = []\n",
        "\n",
        "  for scale in scales:\n",
        "      H, edges = np.histogramdd(pixels, bins=(np.arange(0, Lx, scale), np.arange(0, Ly, scale)))\n",
        "      Ns.append(np.sum(H > 0))\n",
        "\n",
        "  coeffs = np.polyfit(np.log(scales), np.log(Ns), 1)\n",
        "  return -coeffs[0]"
      ],
      "metadata": {
        "id": "knvem1Rupg8g"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DWT"
      ],
      "metadata": {
        "id": "-ykV7Yvyb_6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Get_waves(channel1, channel2, channel3,prefix):\n",
        "  dict_entry = {}\n",
        "  # Loop through the channels and calculate energy and entropy for each sub-band at each level\n",
        "  channel_counter = 1\n",
        "  for channel in [channel1, channel2, channel3]:\n",
        "    counter = 1\n",
        "    coeffs = pywt.wavedec2(channel, 'haar', level=3)\n",
        "    first_10_subbands = coeffs[1:10]\n",
        "\n",
        "    dict_entry[prefix+f'channel_{channel_counter}'+f'_a_energy_{counter}'] = calculate_energy_numpy(coeffs[0])\n",
        "    dict_entry[prefix+f'channel_{channel_counter}'+f'_a_entropy_{counter}'] = calculate_entropy_scipy(coeffs[0])\n",
        "\n",
        "    for subband in first_10_subbands:\n",
        "      horizontal_details_level, vertical_details_level, diagonal_details_level = subband\n",
        "\n",
        "      horizontal_energy = calculate_energy_numpy(horizontal_details_level)\n",
        "      vertical_energy = calculate_energy_numpy(vertical_details_level)\n",
        "      diagonal_energy = calculate_energy_numpy(diagonal_details_level)\n",
        "\n",
        "      horizontal_entropy_value = calculate_entropy_scipy(horizontal_details_level)\n",
        "      vertical_entropy_value = calculate_entropy_scipy(vertical_details_level)\n",
        "      diagonal_entropy_value = calculate_entropy_scipy(diagonal_details_level)\n",
        "\n",
        "      #rgb_channel_x_energy_x\n",
        "      dict_entry[prefix+f'channel_{channel_counter}'+f'_h_energy_{counter}'] = horizontal_energy\n",
        "      dict_entry[prefix+f'channel_{channel_counter}'+f'_v_energy_{counter}'] = vertical_energy\n",
        "      dict_entry[prefix+f'channel_{channel_counter}'+f'_d_energy_{counter}'] = diagonal_energy\n",
        "\n",
        "      dict_entry[prefix+f'channel_{channel_counter}'+f'_h_entropy_{counter}'] = horizontal_entropy_value\n",
        "      dict_entry[prefix+f'channel_{channel_counter}'+f'_v_entropy_{counter}'] = vertical_entropy_value\n",
        "      dict_entry[prefix+f'channel_{channel_counter}'+f'_d_entropy_{counter}'] = diagonal_entropy_value\n",
        "\n",
        "      counter = counter + 1\n",
        "    channel_counter = channel_counter + 1\n",
        "\n",
        "  return dict_entry"
      ],
      "metadata": {
        "id": "B6cf6fd-t5Rp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_entropy(sub_band):\n",
        "    # Square the sub-band\n",
        "    squared = np.square(sub_band)\n",
        "\n",
        "    # Calculate the log of the squared sub-band, handling the case where the value is 0\n",
        "    log_squared = np.where(squared != 0, np.log(squared), 0)\n",
        "\n",
        "    # Multiply the squared sub-band by the log of the squared sub-band\n",
        "    product = squared * log_squared\n",
        "\n",
        "    # Sum all the values and divide by the number of elements to get the entropy\n",
        "    entropy_c = np.sum(product) / np.size(sub_band)\n",
        "\n",
        "    return entropy_c"
      ],
      "metadata": {
        "id": "qW8ZJ8sQwPzE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_energy(sub_band):\n",
        "    # Square the sub-band\n",
        "    squared = np.square(sub_band)\n",
        "\n",
        "    # Sum all the values and divide by the number of elements\n",
        "    mean_square = np.sum(squared) / np.size(sub_band)\n",
        "\n",
        "    # Take the square root to get the energy\n",
        "    energy_c = np.sqrt(mean_square)\n",
        "\n",
        "    return energy_c"
      ],
      "metadata": {
        "id": "uvmyyHW3x7KE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_entropy_scipy(sub_band):\n",
        "    # Flatten the sub-band to 1D and calculate the probability distribution\n",
        "    p = np.square(sub_band).flatten()\n",
        "    p /= np.sum(p)\n",
        "\n",
        "    # Calculate the entropy using scipy\n",
        "    entropy_value = entropy(p)\n",
        "\n",
        "    return entropy_value\n",
        "\n",
        "def calculate_energy_numpy(sub_band):\n",
        "    # Calculate the energy (Euclidean norm) using numpy\n",
        "    energy_c = np.linalg.norm(sub_band)\n",
        "\n",
        "    return energy_c"
      ],
      "metadata": {
        "id": "8PKkaRi0x6qU"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (GLCMs)"
      ],
      "metadata": {
        "id": "JuqlUxzvyBsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_variance(glcm):\n",
        "    N = glcm.shape[0]\n",
        "    I, J = np.ogrid[0:N, 0:N]\n",
        "    mu = np.apply_over_axes(np.sum, I * glcm, axes=(0, -1))\n",
        "    variance = np.apply_over_axes(np.sum, (I - mu) ** 2 * glcm, axes=(0, -1))\n",
        "    return variance\n",
        "\n",
        "def calculate_sum_entropy(glcm):\n",
        "    num_levels, num_levels, num_distances, num_angles = glcm.shape\n",
        "    sum_entropy = 0\n",
        "    for d in range(num_distances):\n",
        "        for a in range(num_angles):\n",
        "            glcm_da = glcm[:, :, d, a]\n",
        "            p_x_plus_y = np.array([np.sum(glcm_da[i:i+num_levels, j:j+num_levels]) for i in range(num_levels) for j in range(num_levels)])\n",
        "            sum_entropy += -np.nansum(p_x_plus_y * np.log2(p_x_plus_y + (p_x_plus_y == 0)))\n",
        "    return sum_entropy\n",
        "\n",
        "# Calculate IMC1 (Information Measure of Correlation 1)\n",
        "def calculate_imc1pi(glcm):\n",
        "    N = glcm.shape[0]\n",
        "    I, J = np.ogrid[0:N, 0:N]\n",
        "    hx = np.sum(glcm * np.log2(glcm + np.finfo(float).eps), axis=0)\n",
        "    hy = np.sum(glcm * np.log2(glcm + np.finfo(float).eps), axis=1)\n",
        "    hxy1 = -np.sum(glcm * np.log2(glcm + np.finfo(float).eps))\n",
        "    hxy2 = -np.sum(glcm * np.log2(glcm + np.finfo(float).eps))\n",
        "    imc1 = (hxy1 - hxy2) / np.max(hx, hy)\n",
        "    return imc1\n",
        "\n",
        "# Calculate IMC2 (Information Measure of Correlation 2)\n",
        "def calculate_imc2pi(glcm):\n",
        "    N = glcm.shape[0]\n",
        "    I, J = np.ogrid[0:N, 0:N]\n",
        "    hx = np.sum(glcm * np.log2(glcm + np.finfo(float).eps), axis=0)\n",
        "    hy = np.sum(glcm * np.log2(glcm + np.finfo(float).eps), axis=1)\n",
        "    hxy1 = -np.sum(glcm * np.log2(glcm + np.finfo(float).eps))\n",
        "    hxy2 = -np.sum(glcm * np.log2(glcm + np.finfo(float).eps))\n",
        "    imc2 = (1 - np.exp(-2 * (hxy2 - hxy1))) ** 0.5\n",
        "    return imc2\n",
        "\n",
        "def calculate_difference_entropy(glcm):\n",
        "    diff_entropy = 0\n",
        "    num_directions = glcm.shape[3]\n",
        "\n",
        "    for direction in range(num_directions):\n",
        "        glcm_direction = glcm[:, :, 0, direction]\n",
        "        diff_entropy += -np.sum(glcm_direction * np.log(glcm_direction + (glcm_direction == 0)))\n",
        "\n",
        "    return diff_entropy\n",
        "###\n",
        "# Calculate IMC1 (Information Measure of Correlation 1)\n",
        "def calculate_imc1(glcm):\n",
        "    HXY = -np.sum(glcm * np.log(glcm + (glcm == 0)))\n",
        "    HX = -np.sum(np.sum(glcm, axis=0) * np.log(np.sum(glcm, axis=0) + (np.sum(glcm, axis=0) == 0)))\n",
        "    HY = -np.sum(np.sum(glcm, axis=2) * np.log(np.sum(glcm, axis=2) + (np.sum(glcm, axis=2) == 0)))\n",
        "    IMC1 = (HXY - HX - HY) / max(HX, HY)\n",
        "    return IMC1\n",
        "\n",
        "# Calculate IMC2 (Information Measure of Correlation 2)\n",
        "def calculate_imc2(glcm):\n",
        "    HXY = -np.sum(glcm * np.log(glcm + (glcm == 0)))\n",
        "    HX = -np.sum(np.sum(glcm, axis=0) * np.log(np.sum(glcm, axis=0) + (np.sum(glcm, axis=0) == 0)))\n",
        "    HY = -np.sum(np.sum(glcm, axis=2) * np.log(np.sum(glcm, axis=2) + (np.sum(glcm, axis=2) == 0)))\n",
        "    IMC2 = np.sqrt(1 - np.exp(-2 * (HXY - HX - HY)))\n",
        "    return IMC2\n",
        "\n",
        "###\n",
        "def calculate_difference_entropy_pi(glcm):\n",
        "    N = glcm.shape[0]\n",
        "    I, J = np.ogrid[0:N, 0:N]\n",
        "    diff = np.abs(I - J)\n",
        "    p_diff = np.sum(glcm * (diff == diff[:, None]), axis=0)\n",
        "    diff_entropy = -np.sum(p_diff * np.log2(p_diff + np.finfo(float).eps))\n",
        "    return diff_entropy\n",
        "\n",
        "# Calculate MCC (Maximal Correlation Coefficient)\n",
        "def calculate_mcc(glcm):\n",
        "    C = np.outer(np.sum(glcm, axis=2), np.sum(glcm, axis=0))\n",
        "    U, S, Vt = np.linalg.svd(glcm)\n",
        "    MCC = np.max(S) / np.sqrt(np.sum(C * C))\n",
        "    return MCC"
      ],
      "metadata": {
        "id": "RWAlqmFP14MW"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Calc_GLCMs(channel_names,red_channel,green_channel,blue_channel):\n",
        "  fullname = ''.join(channel_names)\n",
        "  glcm_dict = {}\n",
        "\n",
        "  # Quantize the intensities into 16 levels\n",
        "  red_quantized = img_as_ubyte(red_channel) // 16\n",
        "  green_quantized = img_as_ubyte(green_channel) // 16\n",
        "  blue_quantized = img_as_ubyte(blue_channel) // 16\n",
        "\n",
        "  # List of color channels\n",
        "  channels = [red_quantized, green_quantized, blue_quantized]\n",
        "\n",
        "  for channel, name in zip(channels, channel_names):\n",
        "      # Calculate the GLCM\n",
        "      glcm = graycomatrix(channel, distances=[1], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], levels=16, symmetric=True, normed=True)\n",
        "\n",
        "      # Calculate properties of the GLCM\n",
        "      contrast = np.mean(graycoprops(glcm, 'contrast'))\n",
        "      dissimilarity = np.mean(graycoprops(glcm, 'dissimilarity'))\n",
        "      homogeneity = np.mean(graycoprops(glcm, 'homogeneity')) #inverse difference moment\n",
        "      energy = np.mean(graycoprops(glcm, 'energy')) #angular second moment\n",
        "      correlation = np.mean(graycoprops(glcm, 'correlation'))\n",
        "      glcm_dict[fullname+name+'contrast'] = contrast\n",
        "      glcm_dict[fullname+name+'dissimilarity'] = dissimilarity\n",
        "      glcm_dict[fullname+name+'homogeneity'] = homogeneity\n",
        "      glcm_dict[fullname+name+'energy'] = energy\n",
        "      glcm_dict[fullname+name+'correlation'] = correlation\n",
        "\n",
        "      #variance\n",
        "      variance = np.var(glcm.flatten())\n",
        "\n",
        "      # Entropy\n",
        "      #entropy = -np.sum(glcm * np.log(glcm + (glcm == 0)))\n",
        "      entropy_v = -np.sum(glcm * np.log2(glcm + np.finfo(float).eps))#copilot\n",
        "\n",
        "      #sum entropy\n",
        "      sum_entropy = calculate_sum_entropy(glcm)\n",
        "\n",
        "      # Sum Average\n",
        "      sum_average = np.sum(np.outer(np.arange(2, 2 * len(glcm) + 2), glcm))\n",
        "\n",
        "      # Sum Variance\n",
        "      sum_variance = np.sum(np.outer((np.arange(2, 2 * len(glcm) + 2) - sum_average) ** 2, glcm))\n",
        "\n",
        "      # Difference Variance\n",
        "      diff_variance = np.sum(np.outer(np.arange(len(glcm)) ** 2, glcm))\n",
        "\n",
        "      mcc_value = calculate_mcc(glcm)\n",
        "\n",
        "      imc1_value = calculate_imc1(glcm)\n",
        "\n",
        "      difference_entropy = calculate_difference_entropy(glcm)\n",
        "\n",
        "      imc2_value = calculate_imc2pi(glcm)\n",
        "\n",
        "      glcm_dict[fullname+name+'variance'] = variance\n",
        "      glcm_dict[fullname+name+'entropy'] = entropy_v\n",
        "      glcm_dict[fullname+name+'sum_entropy'] = sum_entropy\n",
        "      glcm_dict[fullname+name+'sum_average'] = sum_average\n",
        "      glcm_dict[fullname+name+'sum_variance'] = sum_variance\n",
        "      glcm_dict[fullname+name+'diff_variance'] = diff_variance\n",
        "      glcm_dict[fullname+name+'mcc_value'] = mcc_value\n",
        "      glcm_dict[fullname+name+'imc1_value'] = imc1_value\n",
        "      glcm_dict[fullname+name+'difference_entropy'] = difference_entropy\n",
        "      glcm_dict[fullname+name+'imc2_value'] = imc2_value\n",
        "  return glcm_dict"
      ],
      "metadata": {
        "id": "Qhh6BB9TyD8H"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_NUM = \"1\"\n",
        "with open('RandomForestClassifier.pkl', 'rb') as file:\n",
        "  loaded_model = pickle.load(file)\n",
        "\n",
        "for folder in folder_names:\n",
        "  img_path = None\n",
        "  mask_path = None\n",
        "  for file_name in os.listdir(os.path.join(root_folder, folder)):\n",
        "    # If a file is an image and has 'mask' in its name, save its path\n",
        "    if fnmatch.fnmatch(file_name, '*mask*.jpg') or fnmatch.fnmatch(file_name, '*mask*.png'):\n",
        "      print(f\"Found image with 'mask' in name: {file_name}\")\n",
        "      mask_path = os.path.join(root_folder, folder, file_name)\n",
        "    # If there's an image file named '1' (with any extension), save its path\n",
        "    if fnmatch.fnmatch(file_name, '1.*'):\n",
        "      print(f\"Found image file named '1': {file_name}\")\n",
        "      img_path = os.path.join(root_folder, folder, file_name)\n",
        "\n",
        "  if(img_path != None and mask_path != None):\n",
        "    dds = process(img_path, mask_path)\n",
        "    dds = dds[dds.columns.drop(list(dds.filter(regex='21')))]#sometimes slic will fail to generate 20, this is nuclear solution\n",
        "    dds.fillna(0, inplace=True)\n",
        "    dds = dds.replace([np.inf, -np.inf], np.finfo(np.float32).max)\n",
        "    dds = dds.where(dds <= np.finfo(np.float32).max, np.finfo(np.float32).max)\n",
        "    y_test = loaded_model.predict(dds)\n",
        "    print(folder + \" is \" + str(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRHy0sMRVJj2",
        "outputId": "a0c5b5d7-bbb7-476b-956b-3f9be4eac194"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found image file named '1': 1.png\n",
            "Found image with 'mask' in name: ISIC_5117966_mask.png\n",
            "['test image is Lentigo maligna']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Uiqh8rx4HJH",
        "outputId": "dd7306df-99b1-4182-d107-d4c2357db5af"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['test image is Lentigo maligna']\n"
          ]
        }
      ]
    }
  ]
}
